{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c325470d-ec78-4e52-b086-0c3d06cf9d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f191edd7-77b7-4379-9738-4dc6823a9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50a45e8-fa90-4f62-8183-d6d84578727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_PATH = \"../dataset/bootcamp/train/images\"\n",
    "TRAIN_MASK_PATH = \"../dataset/bootcamp/train/annotations\"\n",
    "VAL_IMG_PATH = \"../dataset/bootcamp/test/images\"\n",
    "VAL_MASK_PATH = \"../dataset/bootcamp/test/annotations\"\n",
    "\n",
    "EDA_DS_PATH = \"../dataset/bootcamp/train\"\n",
    "EDA_TRAIN_LABELS = os.path.join(EDA_DS_PATH, \"train_labels.csv\")\n",
    "EDA_TRAIN_FILES = os.path.join(EDA_DS_PATH, \"train_files.csv\")\n",
    "EDA_VAL_FILES = os.path.join(EDA_DS_PATH, \"val_files.csv\")\n",
    "\n",
    "MODEL_PATH = \"../models/UNet\"\n",
    "\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)  # Ensure directory exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "249291b9-7568-4f20-ac1c-608235ffc0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df = pd.read_csv(EDA_TRAIN_FILES)\n",
    "labels_df = pd.read_csv(EDA_TRAIN_LABELS)\n",
    "val_files_df = pd.read_csv(EDA_VAL_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15b164c-309e-4f8b-857a-d911e8f70d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename\n",
      "0  0001TP_006690.png\n",
      "1  0001TP_006720.png\n",
      "2  0001TP_006750.png\n",
      "3  0001TP_006780.png\n",
      "4  0001TP_006810.png\n",
      "\n",
      "   class     label\n",
      "0      0       sky\n",
      "1      1  building\n",
      "2      2      pole\n",
      "3      3      road\n",
      "4      4  sidewalk\n",
      "\n",
      "['0001TP_006690.png' '0001TP_006720.png' '0001TP_006750.png'\n",
      " '0001TP_006780.png' '0001TP_006810.png']\n",
      "\n",
      "['0016E5_07959.png' '0016E5_07961.png' '0016E5_07963.png'\n",
      " '0016E5_07965.png' '0016E5_07967.png']\n"
     ]
    }
   ],
   "source": [
    "print(files_df.head())\n",
    "print()\n",
    "print(labels_df.head())\n",
    "print()\n",
    "train_files = files_df['filename'].to_numpy()\n",
    "print(train_files[:5])\n",
    "print()\n",
    "val_files = val_files_df['filename'].to_numpy()\n",
    "print(val_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791697b5-c561-4be5-8403-193faae76ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d9399-856e-4d52-bd22-bc16e0a7fc31",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f8a0fff-04c1-45de-ad46-15405ac1d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to dataset.py\n",
    "from dataset import CityScapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "749b5b80-677e-4df2-993b-c0f97f1279cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.8439, -1.9124, -1.8782,  ..., -1.8268, -1.8610, -1.8268],\n",
      "         [-1.8782, -1.8782, -1.8953,  ..., -1.8268, -1.8610, -1.8268],\n",
      "         [-1.8953, -1.8610, -1.8610,  ..., -1.9124, -1.8782, -1.8439],\n",
      "         ...,\n",
      "         [-1.8610, -1.8610, -1.8439,  ..., -1.7240, -1.6727, -1.6727],\n",
      "         [-1.8439, -1.8439, -1.8439,  ..., -1.6898, -1.6213, -1.7240],\n",
      "         [-1.8439, -1.8610, -1.8439,  ..., -1.7240, -1.7240, -1.7412]],\n",
      "\n",
      "        [[-1.7556, -1.8256, -1.7906,  ..., -1.6681, -1.7031, -1.7206],\n",
      "         [-1.7906, -1.7906, -1.8081,  ..., -1.6681, -1.7031, -1.7206],\n",
      "         [-1.8081, -1.7731, -1.7731,  ..., -1.7556, -1.7206, -1.7381],\n",
      "         ...,\n",
      "         [-1.7556, -1.7556, -1.7381,  ..., -1.4580, -1.4055, -1.3880],\n",
      "         [-1.7381, -1.7381, -1.7381,  ..., -1.4580, -1.3704, -1.4405],\n",
      "         [-1.7556, -1.7556, -1.7381,  ..., -1.4580, -1.4405, -1.4580]],\n",
      "\n",
      "        [[-1.5256, -1.5953, -1.5604,  ..., -1.4210, -1.4559, -1.4384],\n",
      "         [-1.5604, -1.5604, -1.5779,  ..., -1.4210, -1.4559, -1.4559],\n",
      "         [-1.5779, -1.5430, -1.5430,  ..., -1.5081, -1.4733, -1.4733],\n",
      "         ...,\n",
      "         [-1.4907, -1.4907, -1.4733,  ..., -1.1247, -1.0724, -1.1421],\n",
      "         [-1.4733, -1.4733, -1.4733,  ..., -1.1421, -1.0898, -1.1421],\n",
      "         [-1.4733, -1.4907, -1.4733,  ..., -1.1421, -1.1421, -1.1421]]])\n",
      "\n",
      "tensor([[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 4,  4,  4,  ..., 11, 11, 11],\n",
      "        [ 4,  4,  4,  ..., 11, 11, 11],\n",
      "        [ 4,  4,  4,  ..., 11, 11, 11]], dtype=torch.uint8)\n",
      "\n",
      "torch.Size([3, 256, 256])\n",
      "\n",
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "train_datasets = CityScapes(TRAIN_IMG_PATH, TRAIN_MASK_PATH, train_files)\n",
    "image, mask = train_datasets.__getitem__(0)\n",
    "print(image)\n",
    "print()\n",
    "print(mask)\n",
    "print()\n",
    "print(image.shape)\n",
    "print()\n",
    "print(mask.shape)\n",
    "\n",
    "val_datasets = CityScapes(VAL_IMG_PATH, VAL_MASK_PATH, val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca2b79c-0105-476c-a17d-7a8853e49233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_datasets, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_datasets, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e468e0-939f-4de4-870d-08475952464d",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d20aa0a-bf15-474c-8bb8-548d029116d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture import DoubleConv\n",
    "import torch.nn as nn\n",
    "\n",
    "class FCN8s(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, features= [64, 128, 256, 512, 1024]):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "\n",
    "    for feature in features:\n",
    "      self.layers.append(DoubleConv(in_channels, feature))\n",
    "      in_channels = feature\n",
    "\n",
    "    self.ups1 = nn.ConvTranspose2d(features[-1], features[-2], kernel_size=2, stride=2)\n",
    "    self.ups2 = nn.ConvTranspose2d(features[-1], features[-3], kernel_size=2, stride=2)\n",
    "\n",
    "    self.predictions = nn.ConvTranspose2d(features[-2], out_channels, kernel_size=8, stride=8)\n",
    "\n",
    "  def forward(self, x):\n",
    "    skip_connections=[]\n",
    "\n",
    "    for idx,layer in enumerate(self.layers):\n",
    "      x = layer(x)\n",
    "      x = self.pool(x)\n",
    "      if idx in [2,3]:\n",
    "        skip_connections.append(x)\n",
    "\n",
    "\n",
    "    ups1 = self.ups1(x)\n",
    "    concat1 = torch.concat([ups1, skip_connections[-1]], dim=1)\n",
    "\n",
    "    ups2 = self.ups2(concat1)\n",
    "    concat2 = torch.concat([ups2, skip_connections[-2]], dim=1)\n",
    "\n",
    "    return self.predictions(concat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69139016-43ac-4b2f-8a67-8f6b14de3521",
   "metadata": {},
   "source": [
    "## Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ae80138-d237-4d78-88bc-e26d946279a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import DiceLoss\n",
    "from engine import DiceCrossEntropyLoss\n",
    "from engine import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0a9de4a-f655-4aaf-a12e-483664a6efb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FCN8s' object has no attribute 'out_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m loss_fn = DiceLoss()\n\u001b[32m      4\u001b[39m optim = torch.optim.Adam(model.parameters(), lr = \u001b[32m1e-3\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mprint\u001b[39m(model.out_channels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\ai_cv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1929\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1930\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1931\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1932\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1933\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'FCN8s' object has no attribute 'out_channels'"
     ]
    }
   ],
   "source": [
    "model = FCN8s(in_channels=3, out_channels=len(labels_df))\n",
    "model.cuda()\n",
    "loss_fn = DiceLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "print(model.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434200e-a97f-4b8e-8c63-4ceed63e4968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
