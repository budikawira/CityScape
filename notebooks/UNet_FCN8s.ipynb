{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVxGqaMqppnt"
   },
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvM8hwMIproK",
    "outputId": "756308a8-203b-40e7-a2a9-b4815d8e4ce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8cNmPnRXqFT_"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../dataset/bootcamp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfwctuSTpnIy"
   },
   "source": [
    "## Import some Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4Rwt1IEZokg6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxtL9QB1qfyF"
   },
   "source": [
    "## Datasets Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0001TP_006690.png', '0001TP_006720.png', '0001TP_006750.png', '0001TP_006780.png', '0001TP_006810.png', '0001TP_006840.png', '0001TP_006870.png', '0001TP_006900.png', '0001TP_006930.png', '0001TP_006960.png', '0001TP_006990.png', '0001TP_007020.png', '0001TP_007050.png', '0001TP_007080.png', '0001TP_007110.png', '0001TP_007140.png', '0001TP_007170.png', '0001TP_007200.png', '0001TP_007230.png', '0001TP_007260.png', '0001TP_007290.png', '0001TP_007320.png', '0001TP_007350.png', '0001TP_007380.png', '0001TP_007410.png', '0001TP_007440.png', '0001TP_007470.png', '0001TP_007500.png', '0001TP_007530.png', '0001TP_007560.png', '0001TP_007590.png', '0001TP_007620.png', '0001TP_007650.png', '0001TP_007680.png', '0001TP_007710.png', '0001TP_007740.png', '0001TP_007770.png', '0001TP_007800.png', '0001TP_007830.png', '0001TP_007860.png', '0001TP_007890.png', '0001TP_007920.png', '0001TP_007950.png', '0001TP_007980.png', '0001TP_008010.png', '0001TP_008040.png', '0001TP_008070.png', '0001TP_008100.png', '0001TP_008130.png', '0001TP_008160.png', '0001TP_008190.png', '0001TP_008220.png', '0001TP_008250.png', '0001TP_008280.png', '0001TP_008310.png', '0001TP_008340.png', '0001TP_008370.png', '0001TP_008400.png', '0001TP_008430.png', '0001TP_008460.png', '0001TP_008490.png', '0001TP_008520.png', '0006R0_f00930.png', '0006R0_f00960.png', '0006R0_f00990.png', '0006R0_f01020.png', '0006R0_f01050.png', '0006R0_f01080.png', '0006R0_f01110.png', '0006R0_f01140.png', '0006R0_f01170.png', '0006R0_f01200.png', '0006R0_f01230.png', '0006R0_f01260.png', '0006R0_f01290.png', '0006R0_f01320.png', '0006R0_f01350.png', '0006R0_f01380.png', '0006R0_f01410.png', '0006R0_f01440.png', '0006R0_f01470.png', '0006R0_f01500.png', '0006R0_f01530.png', '0006R0_f01560.png', '0006R0_f01590.png', '0006R0_f01620.png', '0006R0_f01650.png', '0006R0_f01680.png', '0006R0_f01710.png', '0006R0_f01740.png', '0006R0_f01770.png', '0006R0_f01800.png', '0006R0_f01830.png', '0006R0_f01860.png', '0006R0_f01890.png', '0006R0_f01920.png', '0006R0_f01950.png', '0006R0_f01980.png', '0006R0_f02010.png', '0006R0_f02040.png', '0006R0_f02070.png', '0006R0_f02100.png', '0006R0_f02130.png', '0006R0_f02160.png', '0006R0_f02190.png', '0006R0_f02220.png', '0006R0_f02250.png', '0006R0_f02280.png', '0006R0_f02310.png', '0006R0_f02340.png', '0006R0_f02370.png', '0006R0_f02400.png', '0006R0_f02430.png', '0006R0_f02460.png', '0006R0_f02490.png', '0006R0_f02520.png', '0006R0_f02550.png', '0006R0_f02580.png', '0006R0_f02610.png', '0006R0_f02640.png', '0006R0_f02670.png', '0006R0_f02700.png', '0006R0_f02730.png', '0006R0_f02760.png', '0006R0_f02790.png', '0006R0_f02820.png', '0006R0_f02850.png', '0006R0_f02880.png', '0006R0_f02910.png', '0006R0_f02940.png', '0006R0_f02970.png', '0006R0_f03000.png', '0006R0_f03030.png', '0006R0_f03060.png', '0006R0_f03090.png', '0006R0_f03120.png', '0006R0_f03150.png', '0006R0_f03180.png', '0006R0_f03210.png', '0006R0_f03240.png', '0006R0_f03270.png', '0006R0_f03300.png', '0006R0_f03330.png', '0006R0_f03360.png', '0006R0_f03390.png', '0006R0_f03420.png', '0006R0_f03450.png', '0006R0_f03480.png', '0006R0_f03510.png', '0006R0_f03540.png', '0006R0_f03570.png', '0006R0_f03600(1).png', '0006R0_f03600.png', '0006R0_f03630(1).png', '0006R0_f03630.png', '0006R0_f03660(1).png', '0006R0_f03660.png', '0006R0_f03690(1).png', '0006R0_f03690.png', '0006R0_f03720(1).png', '0006R0_f03720.png', '0006R0_f03750(1).png', '0006R0_f03750.png', '0006R0_f03780(1).png', '0006R0_f03780.png', '0006R0_f03810(1).png', '0006R0_f03810.png', '0006R0_f03840(1).png', '0006R0_f03840.png', '0006R0_f03870.png', '0006R0_f03900.png', '0006R0_f03930(1).png', '0006R0_f03930.png', '0016E5_00390.png', '0016E5_00420.png', '0016E5_00450.png', '0016E5_00480.png', '0016E5_00510.png', '0016E5_00540.png', '0016E5_00570.png', '0016E5_00600.png', '0016E5_00630.png', '0016E5_00660.png', '0016E5_00690.png', '0016E5_00720.png', '0016E5_00750.png', '0016E5_00780.png', '0016E5_00810.png', '0016E5_00840.png', '0016E5_00870.png', '0016E5_00901.png', '0016E5_00930.png', '0016E5_00960.png', '0016E5_00990.png', '0016E5_01020.png', '0016E5_01050.png', '0016E5_01080.png', '0016E5_01110.png', '0016E5_01140.png', '0016E5_01170.png', '0016E5_01200.png', '0016E5_01230.png', '0016E5_01260.png', '0016E5_01290.png', '0016E5_01320.png', '0016E5_01350.png', '0016E5_01380.png', '0016E5_01410.png', '0016E5_01440.png', '0016E5_01470.png', '0016E5_01500.png', '0016E5_01530.png', '0016E5_01560.png', '0016E5_01590.png', '0016E5_01620.png', '0016E5_01650.png', '0016E5_01680.png', '0016E5_01710.png', '0016E5_01740.png', '0016E5_01770.png', '0016E5_01800.png', '0016E5_01830.png', '0016E5_01860.png', '0016E5_01890.png', '0016E5_01920.png', '0016E5_01950.png', '0016E5_01980.png', '0016E5_02010.png', '0016E5_02040.png', '0016E5_02070.png', '0016E5_02100.png', '0016E5_02130.png', '0016E5_02160.png', '0016E5_02190.png', '0016E5_02220.png', '0016E5_02250.png', '0016E5_02280.png', '0016E5_02310.png', '0016E5_02340.png', '0016E5_02370.png', '0016E5_02400.png', '0016E5_04350.png', '0016E5_04380.png', '0016E5_04410.png', '0016E5_04440.png', '0016E5_04470.png', '0016E5_04500.png', '0016E5_04530.png', '0016E5_04560.png', '0016E5_04590.png', '0016E5_04620.png', '0016E5_04650.png', '0016E5_04680.png', '0016E5_04710.png', '0016E5_04740.png', '0016E5_04770.png', '0016E5_04800.png', '0016E5_04830.png', '0016E5_04860.png', '0016E5_04890.png', '0016E5_04920.png', '0016E5_04950.png', '0016E5_04980.png', '0016E5_05010.png', '0016E5_05040.png', '0016E5_05070.png', '0016E5_05100.png', '0016E5_05130.png', '0016E5_05160.png', '0016E5_05190.png', '0016E5_05220.png', '0016E5_05250.png', '0016E5_05280.png', '0016E5_05310.png', '0016E5_05340.png', '0016E5_05370.png', '0016E5_05400(1).png', '0016E5_05400.png', '0016E5_05430.png', '0016E5_05460.png', '0016E5_05490.png', '0016E5_05520.png', '0016E5_05550.png', '0016E5_05580.png', '0016E5_05610.png', '0016E5_05640.png', '0016E5_05670.png', '0016E5_05700.png', '0016E5_05730.png', '0016E5_05760.png', '0016E5_05790.png', '0016E5_05820.png', '0016E5_05850.png', '0016E5_05880.png', '0016E5_05910.png', '0016E5_05940.png', '0016E5_05970.png', '0016E5_06000.png', '0016E5_06030.png', '0016E5_06060.png', '0016E5_06090.png', '0016E5_06120.png', '0016E5_06150.png', '0016E5_06180.png', '0016E5_06210.png', '0016E5_06240.png', '0016E5_06270.png', '0016E5_06300.png', '0016E5_06330.png', '0016E5_06360.png', '0016E5_06390.png', '0016E5_06420.png', '0016E5_06450.png', '0016E5_06480.png', '0016E5_06510.png', '0016E5_06540.png', '0016E5_06570.png', '0016E5_06600.png', '0016E5_06630.png', '0016E5_06660.png', '0016E5_06690.png', '0016E5_06720.png', '0016E5_06750.png', '0016E5_06780.png', '0016E5_06810.png', '0016E5_06840.png', '0016E5_06870.png', '0016E5_06900.png', '0016E5_06930.png', '0016E5_06960.png', '0016E5_06990.png', '0016E5_07020.png', '0016E5_07050.png', '0016E5_07080.png', '0016E5_07110.png', '0016E5_07140.png', '0016E5_07170.png', '0016E5_07200.png', '0016E5_07230.png', '0016E5_07260.png', '0016E5_07290.png', '0016E5_07320.png', '0016E5_07350.png', '0016E5_07380.png', '0016E5_07410.png', '0016E5_07440.png', '0016E5_07470.png', '0016E5_07500.png', '0016E5_07530.png', '0016E5_07560.png', '0016E5_07590.png', '0016E5_07620(1).png', '0016E5_07620(2).png', '0016E5_07620.png', '0016E5_07650(1).png', '0016E5_07650(2).png', '0016E5_07650.png', '0016E5_07680(1).png', '0016E5_07680(2).png', '0016E5_07680.png', '0016E5_07710(1).png', '0016E5_07710(2).png', '0016E5_07710.png', '0016E5_07740(1).png', '0016E5_07740(2).png', '0016E5_07740.png', '0016E5_07770(1).png', '0016E5_07770(2).png', '0016E5_07770.png', '0016E5_07800(1).png', '0016E5_07800(2).png', '0016E5_07800.png', '0016E5_07830(1).png', '0016E5_07830(2).png', '0016E5_07830.png', '0016E5_07860(1).png', '0016E5_07860(2).png', '0016E5_07860.png', '0016E5_07890.png', '0016E5_07920(1).png', '0016E5_07920(2).png', '0016E5_07920.png', '0016E5_08190.png', '0016E5_08220.png', '0016E5_08250.png', '0016E5_08280.png', '0016E5_08310.png', '0016E5_08340.png', '0016E5_08370.png', '0016E5_08400.png', '0016E5_08430.png', '0016E5_08460.png', '0016E5_08490.png', '0016E5_08520.png', '0016E5_08550.png', '0016E5_08580.png', '0016E5_08610.png', '0016E5_08640.png']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(DATA_PATH)\n",
    "train_path = os.path.join(DATA_PATH, 'train')\n",
    "img_train_path = os.path.join(train_path, 'images')\n",
    "print(os.listdir(img_train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cIKDzX13qYUC"
   },
   "outputs": [],
   "source": [
    "class CityScapes(torch.utils.data.DataLoader):\n",
    "  def __init__(self, files_list):\n",
    "    super().__init__()\n",
    "    self.files_list = files_list\n",
    "  #magic method\n",
    "  def __len__(self): #ada berapa banyak sih datapoint kita yang akan kita train\n",
    "    return len(self.files_list)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    file_name = os.path.join(PATH, self.files_list[idx])\n",
    "    file_img = os.path.join(file_name, f'images/{self.files_list[idx]}.png')\n",
    "\n",
    "    folder_masks = os.path.join(file_name, 'annotations')\n",
    "    file_masks = [os.path.join(folder_masks, mask) for mask in os.listdir(folder_masks)]\n",
    "\n",
    "    image = Image.open(file_img).convert('RGB')\n",
    "    image = np.array(image)\n",
    "\n",
    "    masks = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "    for m in file_masks:\n",
    "      mask = Image.open(m)\n",
    "      mask = np.asarray(mask)\n",
    "      masks += mask\n",
    "\n",
    "    output = data_transforms(image=image, mask=masks)\n",
    "    image = output['image']\n",
    "    masks = output['mask']\n",
    "\n",
    "    return image, masks.unsqueeze(0)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQhZ2-u3q1-p"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBovn13orATR"
   },
   "source": [
    "- DoubleConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZwUgd0T0q_w5"
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8UbChejq4KB"
   },
   "source": [
    "- Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-2xJJ2qpq58l"
   },
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # doblemconv, dobleconv..\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        #upsam, doubleconv, up, ..\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(#0\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))#1\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1] # reversed(skip_connections)\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgnUYnAZrDwM"
   },
   "source": [
    "# FCN8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h3ez7Oc8rDEX"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFCN8s\u001b[39;00m(nn.Module):\n\u001b[32m      2\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels, out_channels, features= [\u001b[32m64\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m512\u001b[39m, \u001b[32m1024\u001b[39m]):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class FCN8s(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, features= [64, 128, 256, 512, 1024]):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    for feature in features:\n",
    "      self.layers.append(DoubleConv(in_channels, feature))\n",
    "      in_channels = feature\n",
    "\n",
    "    self.ups1 = nn.ConvTranspose2d(features[-1], features[-2], kernel_size=2, stride=2)\n",
    "    self.ups2 = nn.ConvTranspose2d(features[-1], features[-3], kernel_size=2, stride=2)\n",
    "\n",
    "    self.predictions = nn.ConvTranspose2d(features[-2], out_channels, kernel_size=8, stride=8)\n",
    "\n",
    "  def forward(self, x):\n",
    "    skip_connections=[]\n",
    "\n",
    "    for idx,layer in enumerate(self.layers):\n",
    "      x = layer(x)\n",
    "      x = self.pool(x)\n",
    "      if idx in [2,3]:\n",
    "        skip_connections.append(x)\n",
    "\n",
    "\n",
    "    ups1 = self.ups1(x)\n",
    "    concat1 = torch.concat([ups1, skip_connections[-1]], dim=1)\n",
    "\n",
    "    ups2 = self.ups2(concat1)\n",
    "    concat2 = torch.concat([ups2, skip_connections[-2]], dim=1)\n",
    "\n",
    "    return self.predictions(concat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUYBnO-iq0SW"
   },
   "source": [
    "## Engine function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFRJ2NbgsoLl"
   },
   "source": [
    "- Dice coefficient metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "n5zHKY1UsnuK"
   },
   "outputs": [],
   "source": [
    "def calculate_dice_coefficient(ground_truth, predicted):\n",
    "    intersection = np.logical_and(ground_truth, predicted)\n",
    "    dice_coefficient = (2 * np.sum(intersection)) / (np.sum(ground_truth) + np.sum(predicted))\n",
    "    return dice_coefficient\n",
    "\n",
    "def calculate_dice_coefficients(ground_truths, predictions):\n",
    "    num_samples = len(ground_truths)\n",
    "    dice_coefficients = np.zeros(num_samples)\n",
    "    for i in range(num_samples):\n",
    "        dice_coefficients[i] = calculate_dice_coefficient(ground_truths[i], predictions[i])\n",
    "    return dice_coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Troo9nfjssci"
   },
   "source": [
    "- training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Z_oZFYpBqxTA"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    #please do training step in this function\n",
    "    model.train()\n",
    "    loss_one_step = 0\n",
    "    loop = tqdm(dataloader)\n",
    "    for data, targets in loop:\n",
    "        data = data.to('cuda')\n",
    "        targets = targets.float().to(device=\"cuda\")\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "    \n",
    "        optim.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        loss_one_step += loss.item()\n",
    "    \n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return loss_one_step / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    #please do evaluation step that calculate evaluation loss and evaluation metrics dice_score_coefficient\n",
    "    model.eval()\n",
    "    loss_one_step = 0\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for data, targets in loop:\n",
    "        data = data.to('cuda')\n",
    "        targets = targets.float().to(device=\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions = model(data)\n",
    "                loss = loss_fn(predictions, targets)\n",
    "        loss_one_step += loss.item()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    return loss_one_step / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7usHljffrTA_"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  #please init everthing in here and do the training process\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7aS2CkUesbqq"
   },
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\"../models\", \"UNet\")\n",
    "os.makedirs(save_dir, exist_ok=True)  # Ensure directory exist\n",
    "def train(train_dataloaders, val_dataloaders, model, loss_fn, optim, num_epochs, log_freq=10, save_best_model=False, best_model_name='best_model.pth', last_model_name='last_model.pth'):\n",
    "    \"\"\"\n",
    "    Train the model for a given number of epochs.\n",
    "    :param train_dataloaders: A dictionary of dataloaders for training and validation.\n",
    "    :param val_dataloaders: A dictionary of dataloaders for validation.\n",
    "    :param model: The model to train.\n",
    "    :param loss_fn: The loss function to use.\n",
    "    :param optim: The optimizer to use.\n",
    "    :param num_epochs: The number of epochs to train for.\n",
    "    :param log_freq: The frequency with which to log training metrics.\n",
    "    :return: The trained model.\n",
    "    \"\"\"\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # best_model_name = os.path.join('drive/MyDrive/UNet/ckpt_save', best_model_name)\n",
    "    # last_model_name = os.path.join('drive/MyDrive/UNet/ckpt_save', last_model_name)\n",
    "    \n",
    "    best_model_name = os.path.join('../models/UNet', best_model_name)\n",
    "    last_model_name = os.path.join('../models/UNet', last_model_name)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_engine(train_dataloaders, model, loss_fn, optim)\n",
    "        val_loss = val_engine(val_dataloaders, model, loss_fn)\n",
    "\n",
    "        is_best = False\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if save_best_model:\n",
    "                best_model = model\n",
    "                torch.save(best_model.state_dict(), best_model_name)\n",
    "                torch.save(model.state_dict(), last_model_name)\n",
    "                is_best = True\n",
    "\n",
    "        if epoch % log_freq == 0:\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "            print('Train Loss: {:.4f}'.format(train_loss))\n",
    "            print('Val Loss: {:.4f}'.format(val_loss))\n",
    "            if is_best:\n",
    "                print(f'✅ Best model saved! (Val Loss: {format(val_loss)})')\n",
    "            print()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = train(train_dataloader, val_dataloader, model, loss_fn, optim, \u001b[32m100\u001b[39m, log_freq=\u001b[32m1\u001b[39m, save_best_model=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "model = train(train_dataloader, val_dataloader, model, loss_fn, optim, 100, log_freq=1, save_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
